# LLM Fine-tuning ê°€ì´ë“œ

ì´ í´ë”ëŠ” **Gemma-3-1B** ëª¨ë¸ì„ ì§„ë¡œ ìƒë‹´ ë„ë©”ì¸ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
LLM_finetunig/
â”œâ”€â”€ gemma31b-final.ipynb          # íŒŒì¸íŠœë‹ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ë…¸íŠ¸ë¶
â”œâ”€â”€ real_final_counseling.jsonl   # ì§„ë¡œ ìƒë‹´ í•™ìŠµ ë°ì´í„°ì…‹ (5,046ê°œ ìƒ˜í”Œ)
â””â”€â”€ README.md                     # ë³¸ ë¬¸ì„œ
```

## ğŸ¯ ëª©ì 

**ì§„ë¡œÂ·ì‹¬ë¦¬ ìƒë‹´ì‚¬** ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” LLMì„ í•™ìŠµì‹œì¼œ, ëŒ€í•™ ì§„ë¡œ ìƒë‹´ íŒŒì´í”„ë¼ì¸ì—ì„œ RAG ëŒ€ì‹  íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

## ğŸ“Š ë°ì´í„°ì…‹

### `real_final_counseling.jsonl`

- **í˜•ì‹**: JSONL (JSON Lines)
- **êµ¬ì¡°**: ChatML í˜•ì‹ì˜ ëŒ€í™” ë°ì´í„°
- **ìƒ˜í”Œ ìˆ˜**: 5,046ê°œ
- **í¬ë§·**:
  ```json
  {
    "messages": [
      {
        "role": "system",
        "content": "ë‹¹ì‹ ì€ ê³µê°ì ì´ê³  ì‹¤ì§ˆì ì¸ ì§„ë¡œÂ·ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤..."
      },
      {
        "role": "user",
        "content": "ìµœê·¼ ì¹œêµ¬ë“¤ì´ ìì‹ ì´ ì¢‹ì•„í•˜ëŠ” ê²ƒê³¼ ì˜í•˜ëŠ” ê²ƒì„ ë°œê²¬í•´..."
      },
      {
        "role": "assistant",
        "content": "ìì‹ ì˜ íŠ¹ì„± íŒŒì•…ì€ ì§„ë¡œì„ íƒì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í† ëŒ€ê°€ ë˜ê¸° ë•Œë¬¸ì—..."
      }
    ]
  }
  ```

### ë°ì´í„° íŠ¹ì§•

- **ë„ë©”ì¸**: ì§„ë¡œ ìƒë‹´, ì‹¬ë¦¬ ìƒë‹´, ëŒ€í•™ ì§„í•™ ìƒë‹´
- **ìŠ¤íƒ€ì¼**: ê³µê°ì ì´ê³  ì‹¤ì§ˆì ì¸ ì¡°ì–¸ ì œê³µ
- **ì£¼ì œ ì˜ˆì‹œ**:
  - ì§„ë¡œ ì„ íƒ ê³ ë¯¼
  - ì¢‹ì•„í•˜ëŠ” ê²ƒ vs ì˜í•˜ëŠ” ê²ƒì˜ ì°¨ì´
  - ê¿ˆì´ ì—†ëŠ” ìƒí™©ì—ì„œì˜ ì§„ë¡œ íƒìƒ‰
  - ë¶€ëª¨ë‹˜ê³¼ì˜ ì§„ë¡œ ê°ˆë“±
  - íŠ¹ìˆ˜ì¤‘í•™êµ ì§„í•™ ìƒë‹´

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ

### ëª¨ë¸
- **Base Model**: `google/gemma-3-1b-it` (Instruction-tuned ë²„ì „)
- **íŒŒì¸íŠœë‹ ë°©ë²•**: LoRA (Low-Rank Adaptation)
- **ì–‘ìí™”**: 4-bit (BitsAndBytesConfig)

### ë¼ì´ë¸ŒëŸ¬ë¦¬
- `transformers`: ëª¨ë¸ ë¡œë”© ë° í•™ìŠµ
- `peft`: LoRA êµ¬í˜„
- `trl`: SFTTrainer (Supervised Fine-Tuning)
- `datasets`: ë°ì´í„°ì…‹ ì²˜ë¦¬
- `bitsandbytes`: ì–‘ìí™” ì§€ì›

## ğŸ“ í•™ìŠµ í”„ë¡œì„¸ìŠ¤

### 1. í™˜ê²½ ì„¤ì •
```python
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install transformers accelerate bitsandbytes sentencepiece peft datasets trl
```

### 2. ëª¨ë¸ ë¡œë”©
- **4-bit ì–‘ìí™”**ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
- `use_fast=False` (Gemma-3 í† í¬ë‚˜ì´ì € ë²„ê·¸ ëŒ€ì‘)
- `pad_token = eos_token` ì„¤ì •

### 3. ë°ì´í„° ì „ì²˜ë¦¬
- **ChatML í˜•ì‹ ë³€í™˜**: Gemma-3 ê³µì‹ í¬ë§·
  ```
  <start_of_turn>system
  {system_content}
  <end_of_turn>
  <start_of_turn>user
  {user_content}
  <end_of_turn>
  <start_of_turn>assistant
  {assistant_content}
  <end_of_turn>
  ```
- **í…ìŠ¤íŠ¸ ì •ì œ**: HTML íƒœê·¸ ì œê±°, ê³µë°± ì •ê·œí™”
- **í† í¬ë‚˜ì´ì§•**: `max_length=2048`, `token_type_ids` ìƒì„±

### 4. LoRA ì„¤ì •
```python
LoraConfig(
    r=32,                    # rank (1B ëª¨ë¸ì€ 16~32 ì¶”ì²œ)
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)
```

### 5. í•™ìŠµ ì„¤ì •
```python
SFTConfig(
    num_train_epochs=10,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,  # ì‹¤ì œ batch size = 8
    learning_rate=2e-4,
    bf16=True,
    optim="paged_adamw_32bit"
)
```

### 6. Early Stopping
- **CustomEarlyStopping** ì½œë°± êµ¬í˜„
- `patience=10`, `min_delta=0.001`
- Train loss ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë…¸íŠ¸ë¶ ì‹¤í–‰
1. Google Colab ë˜ëŠ” ë¡œì»¬ Jupyter í™˜ê²½ì—ì„œ `gemma31b-final.ipynb` ì—´ê¸°
2. HuggingFace ë¡œê·¸ì¸ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)
3. ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰

### ì£¼ìš” ë‹¨ê³„
1. **ì…€ 0-3**: íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
2. **ì…€ 4**: Base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
3. **ì…€ 5-7**: ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
4. **ì…€ 8**: Data Collator ì„¤ì • (Gemma-3 í˜¸í™˜)
5. **ì…€ 9**: LoRA ì„¤ì •
6. **ì…€ 10-11**: Trainer ì„¤ì • ë° í•™ìŠµ ì‹œì‘
7. **ì…€ 12**: `trainer.train()` ì‹¤í–‰
8. **ì…€ 13**: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
9. **ì…€ 14-15**: HuggingFace Hub ì—…ë¡œë“œ (ì„ íƒ)
10. **ì…€ 16-18**: ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸

## ğŸ’¾ ëª¨ë¸ ì €ì¥

### ë¡œì»¬ ì €ì¥
- **ê²½ë¡œ**: `gemma3_lora_output4/` ë˜ëŠ” `gemma3_lora_college2/`
- **í¬í•¨ íŒŒì¼**:
  - `adapter_config.json`
  - `adapter_model.safetensors`
  - `tokenizer_config.json`
  - `tokenizer.json`

### HuggingFace Hub ì—…ë¡œë“œ
```python
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    repo_id="your-username/gemma3-finetuned-counseling",
    folder_path="gemma3_lora_output4",
    commit_message="Upload finetuned LoRA model"
)
```

## ğŸ” ì¶”ë¡  ì‚¬ìš©ë²•

### íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ
```python
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-1b-it",
    quantization_config=bnb_config,
    device_map="auto"
)

ft_model = PeftModel.from_pretrained(
    base_model,
    "gemma3_lora_output4"  # ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” HF Hub ID
)
```

### ì¶”ë¡  ì‹¤í–‰
```python
def infer_ft(query):
    ft_model.eval()
    inp = tokenizer(query, return_tensors="pt").to(ft_model.device)
    
    out = ft_model.generate(
        input_ids=inp["input_ids"],
        attention_mask=inp.get("attention_mask"),
        max_new_tokens=200
    )
    
    return tokenizer.decode(out[0], skip_special_tokens=True)
```

## ğŸ“ˆ í•™ìŠµ ê²°ê³¼

- **í•™ìŠµ ë°ì´í„°**: 5,046ê°œ ìƒ˜í”Œ
- **Trainable Parameters**: 26,091,520 (LoRA ì–´ëŒ‘í„°ë§Œ)
- **Total Batch Size**: 8 (per_device=2 Ã— gradient_accumulation=4)
- **Total Steps**: 6,310 (10 epochs ê¸°ì¤€)

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### LoRA Rank (r)
- **1B ëª¨ë¸**: 16~32 ì¶”ì²œ
- **ë” í° ëª¨ë¸**: 64~128 ê°€ëŠ¥
- **ë©”ëª¨ë¦¬ vs ì„±ëŠ¥**: rankê°€ í´ìˆ˜ë¡ ë” ë§ì€ íŒŒë¼ë¯¸í„° í•™ìŠµ, ë” ë‚˜ì€ ì„±ëŠ¥ ê°€ëŠ¥

### Learning Rate
- **ê¸°ë³¸ê°’**: 2e-4
- **ì¡°ì • ë²”ìœ„**: 1e-4 ~ 5e-4
- **ë„ˆë¬´ ë†’ìœ¼ë©´**: ë¶ˆì•ˆì •í•œ í•™ìŠµ, loss ë°œì‚°
- **ë„ˆë¬´ ë‚®ìœ¼ë©´**: í•™ìŠµ ì†ë„ ì €í•˜

### Batch Size
- **ë©”ëª¨ë¦¬ ì œì•½**: `per_device_train_batch_size=2`
- **ì‹¤ì œ ë°°ì¹˜**: `gradient_accumulation_steps=4`ë¡œ ì´ 8
- **GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**: `gradient_accumulation_steps` ì¦ê°€

## ğŸ› ì£¼ì˜ì‚¬í•­

1. **í† í¬ë‚˜ì´ì € ë²„ê·¸**: `use_fast=False` í•„ìˆ˜
2. **token_type_ids**: Gemma-3ëŠ” ì§€ì›í•˜ì§€ ì•Šì§€ë§Œ, í•™ìŠµ ì‹œ ìƒì„± í•„ìš”
3. **ì–‘ìí™”**: 4-bit ì‚¬ìš© ì‹œ ì¶”ë¡  ì†ë„ í–¥ìƒ, ì •í™•ë„ ì•½ê°„ ì €í•˜ ê°€ëŠ¥
4. **Early Stopping**: Overfitting ë°©ì§€ë¥¼ ìœ„í•´ patience ì¡°ì • ê¶Œì¥

## ğŸ”— ê´€ë ¨ íŒŒì¼

- **ë°ì´í„° ìƒì„±**: `backend/embedding/make_finetune_dataset.py`
- **LangGraph í†µí•©**: `backend/LangGraph/nodes/classify_rag_finetune.py`
- **ëª¨ë¸ ì‚¬ìš©**: `backend/service/main.py`

## ğŸ“š ì°¸ê³  ìë£Œ

- [Gemma-3 ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/google/gemma-3-1b-it)
- [PEFT LoRA ë¬¸ì„œ](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [TRL SFTTrainer ë¬¸ì„œ](https://huggingface.co/docs/trl/sft_trainer)

